# Задача обратной нормализации текста (Inverse Text Normalization)

## 01.09.2021–22.09.2021

Доделывал курсовую работу.

## 18.10.2021–31.10.2021

Рефакторинг кода по задаче ITN, добавление новой функциональности. Код выложен в папке `ITN`.

Разбирался с RNN для решения задачи ITN, понял, что с используемой реализацией были проблемы. Начал переписывать seq2seq RNN.

Изучил больше литературы по задаче ITN ([1](https://arxiv.org/pdf/2108.09889.pdf), [2](https://arxiv.org/pdf/2104.05055.pdf), [3](https://direct.mit.edu/coli/article/45/2/293/1637/Neural-Models-of-Text-Normalization-for-Speech)), начал исправлять текст курсовой работы.

## 01.11.2021–07.11.2021

Разработка экспериментов для исследования моделей, предложенных в рамках курсовой работы над решением задачи ITN. Решил, что для начала попробую следующее:

* Модели №2 и №3 попробую обучать на классах, предсказываемых классификатором, а не на правильных классах.
* Все предложенные модели протестирую с подстановкой правильных классов, а не предсказанных.

Начал реализовывать данные эксперименты.

## 08.11.2021–14.11.2021

Подготовка к докладу + доклад на спецсеминаре в среду (10.11.2021) (тема: [Instance-Conditioned GAN](https://arxiv.org/abs/2109.05070)).

## 15.11.2021–21.11.2021

Завершил реализацию моделей, провел эксперименты. Результаты в таблице:

| Модель      | WER, % | WER, % (val с правильными классами) | WER, % (train с правильными классами) | WER, % (train и val с правильными классами) |
| ----------- | ------ | ----------------------------------- | ------------------------------------- | ------------------------------------------- |
| RNN         | 20.8   |                                     |                                       |                                             |
| Transformer | 13.2   |                                     |                                       |                                             |
| №1          | 10.2   |                                     |                                       |                                             |
| №2          | 11.3   | 11.6                                | 12.5                                  | 11.5                                        |
| №3          | 11.5   | 11.4                                | 13.4                                  | 12.1                                        |
| №4          |        |                                     | 5.5                                   | 2.9                                         |
| №5          |        |                                     | 5.4                                   | 3.0                                         |

Модели №2 и №3 показывают примерно одинаковое качество на обоих валидациях, в то время как модели №2 и №3 (train с правильными классами) показывают снижение качества при валидации с предсказываемыми классами. Поэтому модели №2 и №3 показательны тем, что если в работе модели используется классификатор, то лучше им во время обучения и пользоваться.

* В моделях №4 и №5 классификатор во время обучения не используется, можно ли как-то начать его использовать? Тогда, возможно, будет улучшение качества?

У моделей №4 и №5 высокое расхождение между результатами двух валидаций. Это тоже говорит о том, что моделям во время обучения лучше использовать классификатор, а не известную разметку классов.

Модель №3 все-таки немного более устойчива к перемене в подстановке классов, скорее всего из-за того, что модель №2 затачивается для каждого токена на его класс, а модель №3 работает с интервалами классов (интервал не так сильно привязан к конкретному токену).

## 22.11.2021–28.11.2021

Разобрался и проверил, можно ли нормально подгружать данные в несколько потоков на Windows. Ответ: нет ([1](https://github.com/pytorch/pytorch/issues/12831), [2](https://github.com/pytorch/pytorch/issues/51344)).

Также проверил обучение моделей в Windows под WSL 2 на GPU (слышал, что у кого-то обучение ускорялось). Ускорение не было получено. Заодно проверил, что при запуске экспериментов программа, оформленная в виде модуля, работает быстрее, чем программа, оформленная в jupyter notebook.

## 29.11.2021–05.12.2021

Реализована модель №6, в которой группы токенов одного типа "переводятся" по отдельности, при этом к эмбеддингам токенов каждой группы конкатенируются типовые эмбеддинги из классификатора. Проведены эксперименты, результаты которых в таблице:

| Модель | WER, % | WER, % (val с правильными классами) | WER, %(train с правильными классами) | WER, %(train и val с правильными классами) |
| ------ | ------ | ----------------------------------- | ------------------------------------ | ------------------------------------------ |
| №6     |        |                                     | 7.0                                  | 3.4                                        |

Модель №6 показала результаты хуже, чем модели №4 и №5. Это может означать, что трансформер путается в контекстуальных эмбеддингах для классов и ему проще работать со специальными токенами.

Также тут хочу провести эксперимент, в котором вместо контекстуальных типовых эмбеддингов будут использоваться константные эмбеддинги (типа вектор (1,)) для проверки того, насколько такие сложные типовые эмбеддинги играют роль или все дело в указании на те токены, которые нужно перевести.

Также тут возможно стоит провести эксперимент, в котором группы токенов никаким образом не выделяются, чтобы проверить, что информация о переводимых группах важна для модели (очевидно, модель будет плохо справляться).

## 06.12.2021–12.12.2021

Разработка новых идей по научной работе:

* Во всех моделях выделения групп токенов (особенно в №№4-6, в №№2-3 это сложнее сделать) можно границы группы немного варьировать влево-вправо для более устойчивого обучения (возникает устойчивость к ошибкам классификатора).
* Модель как модели 4-6, только группы токенов, которые надо перевести, помечаем просто единичкой (например, конкатенируем ее или добавляем). Это более легкая модель + проверим, насколько модели важны всякие навороты с указанием конкретного типа эмбеддингов.
* Можно попробовать упростить классификатор (лог.регрессия, градиентный бустинг и т.п.; такое подойдет только для моделей №№2-5) или наоборот усложнить (BERT, ...).
* Еще вариант — вход в трансформер имеет вид (было "я дам тебе 100 рублей завтра в 10 утра") → "[TRANSLATE] я дам тебе [MONEY] 100 [MONEY] рублей завтра в 10 утра [SEP] 100 рублей" + то же самое для времени ("10 утра"). Тут [TRANSLATE] не обязателен, потому что модель решает всего одну задачу.
* Обучаем трансформер на две задачи: классификация токенов и перевод. Например, (было "я дам тебе 100 рублей завтра в 10 утра") → "[CLS] я дам тебе 100 рублей завтра в 10 утра" → получили классы или классовые эмбеддинги → "[TRANSLATE] я дам тебе [MONEY] 100 [MONEY] рублей завтра в 10 утра [SEP] 100 рублей" + то же самое для времени ("10 утра").
* Обучаем трансформер на два лосса сразу — энкодер должен уметь классифицировать эмбеддинги правильно, а энкодер + декодер — переводить текст.
* Такой же подход, как и модель №1, но классификатор отдельно, а типовые эмбеддинги учатся отдельно; их можно инициализировать выученными эмбеддингами из классификатора; такой подход позволяет использовать любой классификатор, главное, чтобы он выдавал классы и все!

* Проведение экспериментов в том числе и на наборе данных на английском языке.

## 13.12.2021–26.12.2021

Подготовка к отчету по НИР, отчет по НИР (23.12.2021).
